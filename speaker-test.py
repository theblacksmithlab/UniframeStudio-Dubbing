# test!
import json
import librosa
import numpy as np
from pathlib import Path
import argparse
import warnings
from scipy import stats

warnings.filterwarnings("ignore")


def extract_voice_features(audio_path: str, start_time: float, end_time: float, sr: int = 16000):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –≥–æ–ª–æ—Å–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ–ª–∞

    Args:
        audio_path: –ø—É—Ç—å –∫ –∞—É–¥–∏–æ —Ñ–∞–π–ª—É
        start_time: –Ω–∞—á–∞–ª–æ —Å–µ–≥–º–µ–Ω—Ç–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        end_time: –∫–æ–Ω–µ—Ü —Å–µ–≥–º–µ–Ω—Ç–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        sr: —á–∞—Å—Ç–æ—Ç–∞ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏

    Returns:
        dict: –Ω–∞–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ–ª–∞
    """
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ
        y, sr = librosa.load(audio_path, sr=sr)

        # –í—ã—Ä–µ–∑–∞–µ–º –Ω—É–∂–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç
        start_sample = int(start_time * sr)
        end_sample = int(end_time * sr)
        segment = y[start_sample:end_sample]

        if len(segment) < sr * 0.1:  # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Å–µ–≥–º–µ–Ω—Ç
            return None

        # 1. –û–°–ù–û–í–ù–ê–Ø –ß–ê–°–¢–û–¢–ê (F0)
        f0 = librosa.yin(segment, fmin=50, fmax=400, sr=sr)
        voiced_f0 = f0[f0 > 0]

        if len(voiced_f0) == 0:
            return None

        f0_mean = np.mean(voiced_f0)
        f0_median = np.median(voiced_f0)
        f0_std = np.std(voiced_f0)
        f0_range = np.max(voiced_f0) - np.min(voiced_f0)

        # 2. –°–ü–ï–ö–¢–†–ê–õ–¨–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò
        # –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π —Ü–µ–Ω—Ç—Ä–æ–∏–¥ (—Ü–µ–Ω—Ç—Ä –º–∞—Å—Å —Å–ø–µ–∫—Ç—Ä–∞)
        spectral_centroids = librosa.feature.spectral_centroid(y=segment, sr=sr)[0]
        centroid_mean = np.mean(spectral_centroids)

        # –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–∞—è —à–∏—Ä–∏–Ω–∞ –ø–æ–ª–æ—Å—ã (bandwidth)
        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=segment, sr=sr)[0]
        bandwidth_mean = np.mean(spectral_bandwidth)

        # –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π rolloff (—á–∞—Å—Ç–æ—Ç–∞, –Ω–∏–∂–µ –∫–æ—Ç–æ—Ä–æ–π 85% —ç–Ω–µ—Ä–≥–∏–∏)
        spectral_rolloff = librosa.feature.spectral_rolloff(y=segment, sr=sr)[0]
        rolloff_mean = np.mean(spectral_rolloff)

        # 3. –úFCC –ü–†–ò–ó–ù–ê–ö–ò
        mfccs = librosa.feature.mfcc(y=segment, sr=sr, n_mfcc=13)
        mfcc_mean = np.mean(mfccs, axis=1)

        # 4. –§–û–†–ú–ê–ù–¢–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–µ)
        # –ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º—É
        D = librosa.stft(segment)
        magnitude = np.abs(D)

        # –ò—â–µ–º –ø–∏–∫–∏ –≤ —Å–ø–µ–∫—Ç—Ä–µ (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–æ—Ä–º–∞–Ω—Ç—ã)
        freq_bins = librosa.fft_frequencies(sr=sr)
        avg_magnitude = np.mean(magnitude, axis=1)

        # –ù–∞—Ö–æ–¥–∏–º –ø–∏–∫–∏ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ —Ñ–æ—Ä–º–∞–Ω—Ç
        from scipy.signal import find_peaks
        peaks, _ = find_peaks(avg_magnitude, height=np.max(avg_magnitude) * 0.1)
        formant_freqs = freq_bins[peaks]

        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 3 —Ñ–æ—Ä–º–∞–Ω—Ç—ã
        f1 = formant_freqs[0] if len(formant_freqs) > 0 else 0
        f2 = formant_freqs[1] if len(formant_freqs) > 1 else 0
        f3 = formant_freqs[2] if len(formant_freqs) > 2 else 0

        # 5. –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò
        # –î–∂–∏—Ç—Ç–µ—Ä (–≤–∞—Ä–∏–∞—Ü–∏—è –ø–µ—Ä–∏–æ–¥–∞)
        if len(voiced_f0) > 1:
            periods = 1.0 / voiced_f0
            jitter = np.std(periods) / np.mean(periods) if np.mean(periods) > 0 else 0
        else:
            jitter = 0

        # –®–∏–º–º–µ—Ä (–≤–∞—Ä–∏–∞—Ü–∏—è –∞–º–ø–ª–∏—Ç—É–¥—ã)
        rms = librosa.feature.rms(y=segment)[0]
        shimmer = np.std(rms) / np.mean(rms) if np.mean(rms) > 0 else 0

        # –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –≥–æ–ª–æ—Å/—à—É–º
        voiced_ratio = len(voiced_f0) / len(f0)

        return {
            # –û—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞
            "f0_mean": float(f0_mean),
            "f0_median": float(f0_median),
            "f0_std": float(f0_std),
            "f0_range": float(f0_range),

            # –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            "spectral_centroid": float(centroid_mean),
            "spectral_bandwidth": float(bandwidth_mean),
            "spectral_rolloff": float(rolloff_mean),

            # MFCC
            "mfcc_1": float(mfcc_mean[0]),
            "mfcc_2": float(mfcc_mean[1]),
            "mfcc_3": float(mfcc_mean[2]),

            # –§–æ—Ä–º–∞–Ω—Ç—ã
            "f1": float(f1),
            "f2": float(f2),
            "f3": float(f3),

            # –ö–∞—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ—Å–∞
            "jitter": float(jitter),
            "shimmer": float(shimmer),
            "voiced_ratio": float(voiced_ratio),
        }

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ {start_time}-{end_time}: {e}")
        return None


def improved_gender_classification(features):
    """
    –£–ª—É—á—à–µ–Ω–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ–ª–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

    Args:
        features: —Å–ª–æ–≤–∞—Ä—å —Å –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏

    Returns:
        tuple: (predicted_gender, confidence_score)
    """
    if not features:
        return "unknown", 0.0

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—á–µ—Ç—á–∏–∫–∏
    male_score = 0
    female_score = 0
    total_weight = 0

    # 1. –û–°–ù–û–í–ù–ê–Ø –ß–ê–°–¢–û–¢–ê (F0) - –æ—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–∏–∑–Ω–∞–∫
    f0_median = features["f0_median"]
    f0_weight = 0.4

    if f0_median < 110:
        male_score += f0_weight * 1.0
    elif f0_median < 130:
        male_score += f0_weight * 0.8
    elif f0_median < 150:
        male_score += f0_weight * 0.4
    elif f0_median < 170:
        # –°–µ—Ä–∞—è –∑–æ–Ω–∞ - –ø–æ–ª–∞–≥–∞–µ–º—Å—è –Ω–∞ –¥—Ä—É–≥–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        pass
    elif f0_median < 200:
        female_score += f0_weight * 0.4
    elif f0_median < 230:
        female_score += f0_weight * 0.8
    else:
        female_score += f0_weight * 1.0

    total_weight += f0_weight

    # 2. –°–ü–ï–ö–¢–†–ê–õ–¨–ù–´–ô –¶–ï–ù–¢–†–û–ò–î
    centroid = features["spectral_centroid"]
    centroid_weight = 0.2

    if centroid < 1500:
        male_score += centroid_weight * 0.8
    elif centroid < 2000:
        male_score += centroid_weight * 0.4
    elif centroid < 2500:
        pass  # –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è –∑–æ–Ω–∞
    elif centroid < 3000:
        female_score += centroid_weight * 0.4
    else:
        female_score += centroid_weight * 0.8

    total_weight += centroid_weight

    # 3. –§–û–†–ú–ê–ù–¢–´ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã)
    if features["f1"] > 0 and features["f2"] > 0:
        f1, f2 = features["f1"], features["f2"]
        formant_weight = 0.2

        # –ñ–µ–Ω—Å–∫–∏–µ —Ñ–æ—Ä–º–∞–Ω—Ç—ã –æ–±—ã—á–Ω–æ –≤—ã—à–µ
        if f1 > 500 and f2 > 1500:
            female_score += formant_weight * 0.6
        elif f1 < 400 and f2 < 1200:
            male_score += formant_weight * 0.6

        total_weight += formant_weight

    # 4. –í–ê–†–ò–ê–¢–ò–í–ù–û–°–¢–¨ F0
    f0_std = features["f0_std"]
    variability_weight = 0.1

    # –ñ–µ–Ω—Å–∫–∏–µ –≥–æ–ª–æ—Å–∞ –æ–±—ã—á–Ω–æ –±–æ–ª–µ–µ –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω—ã
    if f0_std > 20:
        female_score += variability_weight * 0.5
    elif f0_std < 10:
        male_score += variability_weight * 0.5

    total_weight += variability_weight

    # 5. MFCC –ü–†–ò–ó–ù–ê–ö–ò
    mfcc_weight = 0.1
    if features["mfcc_2"] > 0:
        female_score += mfcc_weight * 0.3
    else:
        male_score += mfcc_weight * 0.3

    total_weight += mfcc_weight

    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –æ—Ü–µ–Ω–∫–∏
    if total_weight > 0:
        male_score /= total_weight
        female_score /= total_weight

    # –ü—Ä–∏–Ω–∏–º–∞–µ–º —Ä–µ—à–µ–Ω–∏–µ
    if male_score > female_score:
        confidence = male_score - female_score
        return "male", min(confidence, 1.0)
    elif female_score > male_score:
        confidence = female_score - male_score
        return "female", min(confidence, 1.0)
    else:
        return "uncertain", 0.3


def analyze_improved_gender(audio_path: str, segments: list, output_path: str = None):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–æ–ª —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –∞–ª–≥–æ—Ä–∏—Ç–º–æ–º
    """
    try:
        print(f"üéµ –£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ–ª–∞ –¥–ª—è {len(segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤...")

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —Å–µ–≥–º–µ–Ω—Ç
        for i, segment in enumerate(segments):
            if i % 10 == 0:
                print(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {i}/{len(segments)}")

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            features = extract_voice_features(
                audio_path,
                segment["start"],
                segment["end"]
            )

            if features:
                # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –ø–æ–ª
                gender, confidence = improved_gender_classification(features)

                # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                segment["voice_features"] = features
                segment["predicted_gender"] = gender
                segment["gender_confidence"] = confidence

                # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ
                segment["gender_explanation"] = {
                    "f0_hz": features["f0_median"],
                    "spectral_centroid": features["spectral_centroid"],
                    "primary_indicators": []
                }

                # –û–±—ä—è—Å–Ω—è–µ–º —Ä–µ—à–µ–Ω–∏–µ
                if features["f0_median"] < 130:
                    segment["gender_explanation"]["primary_indicators"].append("–Ω–∏–∑–∫–∞—è F0")
                elif features["f0_median"] > 200:
                    segment["gender_explanation"]["primary_indicators"].append("–≤—ã—Å–æ–∫–∞—è F0")

                if features["spectral_centroid"] > 2500:
                    segment["gender_explanation"]["primary_indicators"].append("–≤—ã—Å–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π —Ü–µ–Ω—Ç—Ä–æ–∏–¥")
                elif features["spectral_centroid"] < 1500:
                    segment["gender_explanation"]["primary_indicators"].append("–Ω–∏–∑–∫–∏–π —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π —Ü–µ–Ω—Ç—Ä–æ–∏–¥")

            else:
                segment["predicted_gender"] = "unknown"
                segment["gender_confidence"] = 0.0
                segment["voice_features"] = None

        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        print("\n" + "=" * 60)
        print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –£–õ–£–ß–®–ï–ù–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê –ü–û–õ–ê")
        print("=" * 60)

        gender_stats = {"male": 0, "female": 0, "uncertain": 0, "unknown": 0}
        high_conf_stats = {"male": 0, "female": 0}

        for segment in segments:
            gender = segment["predicted_gender"]
            confidence = segment["gender_confidence"]

            gender_stats[gender] += 1

            if confidence > 0.6:
                if gender in ["male", "female"]:
                    high_conf_stats[gender] += 1

        for gender, count in gender_stats.items():
            percentage = (count / len(segments)) * 100
            print(f"{gender.upper()}: {count} —Å–µ–≥–º–µ–Ω—Ç–æ–≤ ({percentage:.1f}%)")

        print(f"\n–í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (>0.6):")
        for gender, count in high_conf_stats.items():
            print(f"  {gender.upper()}: {count} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã
        print(f"\nüìã –ü—Ä–∏–º–µ—Ä—ã —Å –æ–±—ä—è—Å–Ω–µ–Ω–∏—è–º–∏:")
        for i, segment in enumerate(segments[:8]):
            if segment["predicted_gender"] != "unknown":
                expl = segment.get("gender_explanation", {})
                indicators = ", ".join(expl.get("primary_indicators", []))
                print(f"  {i + 1}. {segment['start']:5.1f}s-{segment['end']:5.1f}s | "
                      f"F0: {expl.get('f0_hz', 0):.1f}Hz | "
                      f"{segment['predicted_gender']} ({segment['gender_confidence']:.2f}) | "
                      f"{indicators}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if output_path:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(segments, f, indent=2, ensure_ascii=False)
            print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: {output_path}")

        return segments

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        return None


def main():
    parser = argparse.ArgumentParser(description="–£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ–ª–∞")
    parser.add_argument("audio_path", help="–ü—É—Ç—å –∫ –∞—É–¥–∏–æ —Ñ–∞–π–ª—É")
    parser.add_argument("segments_path", help="–ü—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É —Å —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏")
    parser.add_argument("-o", "--output", help="–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞")

    args = parser.parse_args()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤
    if not Path(args.audio_path).exists():
        print(f"‚ùå –ê—É–¥–∏–æ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {args.audio_path}")
        return

    if not Path(args.segments_path).exists():
        print(f"‚ùå –§–∞–π–ª —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω: {args.segments_path}")
        return

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã
    with open(args.segments_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    segments = data.get("segments", data) if isinstance(data, dict) else data

    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ–ª
    result = analyze_improved_gender(args.audio_path, segments, args.output)

    if result:
        print("\n‚úÖ –£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω!")
        print("üí° –¢–µ–ø–µ—Ä—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —É—á–∏—Ç—ã–≤–∞–µ—Ç –±–æ–ª—å—à–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")


if __name__ == "__main__":
    main()