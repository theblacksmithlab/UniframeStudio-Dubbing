#!/usr/bin/env python3
"""
–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–∫—Ä–∏–ø—Ç —Å –º–æ–¥–µ–ª—å—é pyannote/speaker-diarization (–±–µ–∑ –≤–µ—Ä—Å–∏–∏)
"""

import json
import torch
import torchaudio
import warnings
from pathlib import Path
import argparse

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
warnings.filterwarnings("ignore")


def test_speaker_diarization_alternative(audio_path: str, token: str, output_path: str = None):
    """
    –¢–µ—Å—Ç–∏—Ä—É–µ—Ç speaker diarization —Å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é
    """
    try:
        from pyannote.audio import Pipeline

        print(f"üéµ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –∞—É–¥–∏–æ: {audio_path}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞
        if not Path(audio_path).exists():
            raise FileNotFoundError(f"–ê—É–¥–∏–æ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {audio_path}")

        # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å (–∫–æ—Ç–æ—Ä–∞—è —É –≤–∞—Å —É–∂–µ —Ä–∞–±–æ—Ç–∞–ª–∞)
        print("üì• –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å pyannote/speaker-diarization...")
        pipeline = Pipeline.from_pretrained(
            "pyannote/speaker-diarization",
            use_auth_token=token
        )

        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞ GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        pipeline.to(device)
        print(f"üîß –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

        # –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑
        print("üîÑ –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Å–ø–∏–∫–µ—Ä–æ–≤... (—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç)")
        diarization = pipeline(audio_path)

        print("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω! –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã...")

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        speakers_timeline = []
        for turn, _, speaker in diarization.itertracks(yield_label=True):
            speakers_timeline.append({
                "start": float(turn.start),
                "end": float(turn.end),
                "speaker": speaker,
                "duration": float(turn.end - turn.start)
            })

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        speakers_timeline.sort(key=lambda x: x["start"])

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        speakers_found = sorted(list(set([s["speaker"] for s in speakers_timeline])))
        total_duration = max([s["end"] for s in speakers_timeline]) if speakers_timeline else 0

        result = {
            "audio_file": audio_path,
            "model": "pyannote/speaker-diarization",
            "total_duration": total_duration,
            "speakers_count": len(speakers_found),
            "speakers_found": speakers_found,
            "total_segments": len(speakers_timeline),
            "timeline": speakers_timeline
        }

        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        print("\n" + "=" * 60)
        print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ SPEAKER DIARIZATION")
        print("=" * 60)
        print(f"–§–∞–π–ª: {audio_path}")
        print(f"–ú–æ–¥–µ–ª—å: pyannote/speaker-diarization")
        print(f"–û–±—â–∞—è –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {total_duration:.2f} —Å–µ–∫")
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ø–∏–∫–µ—Ä–æ–≤: {len(speakers_found)}")
        print(f"–ù–∞–π–¥–µ–Ω–Ω—ã–µ —Å–ø–∏–∫–µ—Ä—ã: {', '.join(speakers_found)}")
        print(f"–í—Å–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(speakers_timeline)}")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º
        print(f"\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º:")
        for speaker in speakers_found:
            speaker_segments = [s for s in speakers_timeline if s['speaker'] == speaker]
            speaker_time = sum([s['duration'] for s in speaker_segments])
            percentage = (speaker_time / total_duration) * 100 if total_duration > 0 else 0
            print(f"  {speaker}: {speaker_time:.1f}s ({percentage:.1f}%) –≤ {len(speaker_segments)} —Å–µ–≥–º–µ–Ω—Ç–∞—Ö")

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã
        print(f"\nüìã –ü–µ—Ä–≤—ã–µ 15 —Å–µ–≥–º–µ–Ω—Ç–æ–≤:")
        for i, segment in enumerate(speakers_timeline[:15]):
            print(f"  {i + 1:2d}. {segment['start']:7.2f}s - {segment['end']:7.2f}s "
                  f"({segment['duration']:5.2f}s) | {segment['speaker']}")

        if len(speakers_timeline) > 15:
            print(f"  ... –∏ –µ—â—ë {len(speakers_timeline) - 15} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if output_path:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: {output_path}")

        return result

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        return None


def main():
    parser = argparse.ArgumentParser(description="Speaker Diarization (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å)")
    parser.add_argument("audio_path", help="–ü—É—Ç—å –∫ –∞—É–¥–∏–æ —Ñ–∞–π–ª—É")
    parser.add_argument("--token", required=True, help="HuggingFace —Ç–æ–∫–µ–Ω")
    parser.add_argument("-o", "--output", help="–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è JSON —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞")

    args = parser.parse_args()

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º diarization
    result = test_speaker_diarization_alternative(
        audio_path=args.audio_path,
        token=args.token,
        output_path=args.output
    )

    if result:
        print("\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω —É—Å–ø–µ—à–Ω–æ!")
        print(f"üìä –ù–∞–π–¥–µ–Ω–æ {result['speakers_count']} —Å–ø–∏–∫–µ—Ä–æ–≤ –≤ {result['total_segments']} —Å–µ–≥–º–µ–Ω—Ç–∞—Ö")
        print("\nüîó –†–µ–∑—É–ª—å—Ç–∞—Ç –≥–æ—Ç–æ–≤ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤ –≤–∞—à python-dubbing-service")
    else:
        print("\n‚ùå –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω —Å –æ—à–∏–±–∫–æ–π")


if __name__ == "__main__":
    main()