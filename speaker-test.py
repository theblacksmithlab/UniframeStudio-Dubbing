#!/usr/bin/env python3
"""
–ù–∞–¥–µ–∂–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è speaker diarization —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
"""

import json
import torch
import warnings
from pathlib import Path
import argparse
import sys
import os

# –û—Ç–∫–ª—é—á–∞–µ–º –≤—Å–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
warnings.filterwarnings("ignore")
os.environ['PYTHONWARNINGS'] = 'ignore'


def test_speaker_diarization_robust(audio_path: str, token: str, output_path: str = None):
    """
    –ù–∞–¥–µ–∂–Ω—ã–π —Ç–µ—Å—Ç speaker diarization
    """
    pipeline = None

    try:
        from pyannote.audio import Pipeline

        print(f"üéµ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –∞—É–¥–∏–æ: {audio_path}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞
        if not Path(audio_path).exists():
            raise FileNotFoundError(f"–ê—É–¥–∏–æ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {audio_path}")

        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏
        models = [
            "pyannote/speaker-diarization-3.1",
            "pyannote/speaker-diarization",
            "pyannote/speaker-diarization-3.0"
        ]

        for model_name in models:
            print(f"üì• –ü—Ä–æ–±—É—é –∑–∞–≥—Ä—É–∑–∏—Ç—å: {model_name}")

            try:
                # –ü–æ–¥–∞–≤–ª—è–µ–º stderr –≤–æ –≤—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏
                import contextlib
                with contextlib.redirect_stderr(open(os.devnull, 'w')):
                    pipeline = Pipeline.from_pretrained(
                        model_name,
                        use_auth_token=token
                    )

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ pipeline –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∑–∞–≥—Ä—É–∑–∏–ª—Å—è
                if pipeline is not None:
                    print(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
                    break
                else:
                    print(f"‚ùå –ú–æ–¥–µ–ª—å {model_name} –≤–µ—Ä–Ω—É–ª–∞ None")

            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {model_name}: {str(e)[:100]}...")
                pipeline = None
                continue

        # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
        if pipeline is None:
            print("\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω—É –º–æ–¥–µ–ª—å!")
            print("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ:")
            print("1. –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—é: huggingface-cli login")
            print("2. –ü—Ä–∏–Ω—è—Ç–∏–µ —É—Å–ª–æ–≤–∏–π –Ω–∞ —Å–∞–π—Ç–µ HuggingFace")
            print("3. –ü—Ä–∞–≤–∞ –¥–æ—Å—Ç—É–ø–∞ —Ç–æ–∫–µ–Ω–∞")
            return None

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞ GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        pipeline.to(device)
        print(f"üîß –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

        # –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑
        print("üîÑ –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Å–ø–∏–∫–µ—Ä–æ–≤... (—ç—Ç–æ –∑–∞–π–º–µ—Ç –≤—Ä–µ–º—è)")
        diarization = pipeline(audio_path)

        print("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω! –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã...")

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        speakers_timeline = []
        for turn, _, speaker in diarization.itertracks(yield_label=True):
            speakers_timeline.append({
                "start": float(turn.start),
                "end": float(turn.end),
                "speaker": speaker,
                "duration": float(turn.end - turn.start)
            })

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        speakers_timeline.sort(key=lambda x: x["start"])

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        speakers_found = sorted(list(set([s["speaker"] for s in speakers_timeline])))
        total_duration = max([s["end"] for s in speakers_timeline]) if speakers_timeline else 0

        result = {
            "audio_file": audio_path,
            "model_used": model_name,
            "total_duration": total_duration,
            "speakers_count": len(speakers_found),
            "speakers_found": speakers_found,
            "total_segments": len(speakers_timeline),
            "timeline": speakers_timeline
        }

        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        print("\n" + "=" * 60)
        print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ SPEAKER DIARIZATION")
        print("=" * 60)
        print(f"–§–∞–π–ª: {audio_path}")
        print(f"–ú–æ–¥–µ–ª—å: {model_name}")
        print(f"–û–±—â–∞—è –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {total_duration:.2f} —Å–µ–∫")
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ø–∏–∫–µ—Ä–æ–≤: {len(speakers_found)}")
        print(f"–ù–∞–π–¥–µ–Ω–Ω—ã–µ —Å–ø–∏–∫–µ—Ä—ã: {', '.join(speakers_found)}")
        print(f"–í—Å–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(speakers_timeline)}")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º
        print(f"\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º:")
        for speaker in speakers_found:
            speaker_segments = [s for s in speakers_timeline if s['speaker'] == speaker]
            speaker_time = sum([s['duration'] for s in speaker_segments])
            percentage = (speaker_time / total_duration) * 100 if total_duration > 0 else 0
            print(f"  {speaker}: {speaker_time:.1f}s ({percentage:.1f}%) –≤ {len(speaker_segments)} —Å–µ–≥–º–µ–Ω—Ç–∞—Ö")

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã
        print(f"\nüìã –ü–µ—Ä–≤—ã–µ 15 —Å–µ–≥–º–µ–Ω—Ç–æ–≤:")
        for i, segment in enumerate(speakers_timeline[:15]):
            print(f"  {i + 1:2d}. {segment['start']:7.2f}s - {segment['end']:7.2f}s "
                  f"({segment['duration']:5.2f}s) | {segment['speaker']}")

        if len(speakers_timeline) > 15:
            print(f"  ... –∏ –µ—â—ë {len(speakers_timeline) - 15} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if output_path:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: {output_path}")

        return result

    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        return None


def main():
    parser = argparse.ArgumentParser(description="–ù–∞–¥–µ–∂–Ω—ã–π Speaker Diarization —Ç–µ—Å—Ç")
    parser.add_argument("audio_path", help="–ü—É—Ç—å –∫ –∞—É–¥–∏–æ —Ñ–∞–π–ª—É")
    parser.add_argument("--token", required=True, help="HuggingFace —Ç–æ–∫–µ–Ω")
    parser.add_argument("-o", "--output", help="–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è JSON —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞")

    args = parser.parse_args()

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º diarization
    result = test_speaker_diarization_robust(
        audio_path=args.audio_path,
        token=args.token,
        output_path=args.output
    )

    if result:
        print("\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω —É—Å–ø–µ—à–Ω–æ!")
        print(f"üìä –ù–∞–π–¥–µ–Ω–æ {result['speakers_count']} —Å–ø–∏–∫–µ—Ä–æ–≤ –≤ {result['total_segments']} —Å–µ–≥–º–µ–Ω—Ç–∞—Ö")
        print("\nüîó –†–µ–∑—É–ª—å—Ç–∞—Ç –≥–æ—Ç–æ–≤ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏!")
    else:
        print("\n‚ùå –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω —Å –æ—à–∏–±–∫–æ–π")


if __name__ == "__main__":
    main()